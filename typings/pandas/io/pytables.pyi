"""
This type stub file was generated by pyright.
"""

import itertools
import numpy as np
from typing import Any, Final, Literal, TYPE_CHECKING
from pandas.util._decorators import cache_readonly
from pandas import DataFrame, Index, MultiIndex, Series
from pandas.core.computation.pytables import PyTablesExpr
from collections.abc import Hashable, Iterator
from types import TracebackType
from tables import Col, File, Node
from pandas._typing import AnyArrayLike, ArrayLike, AxisInt, DtypeArg, FilePath, Self, Shape, npt

"""
High level interface to PyTables for reading and writing pandas data structures
to disk
"""
if TYPE_CHECKING:
    ...
_version = ...
_default_encoding = ...
Term = PyTablesExpr
incompatibility_doc: Final = ...
attribute_conflict_doc: Final = ...
performance_doc: Final = ...
_FORMAT_MAP = ...
_AXES_MAP = ...
dropna_doc: Final = ...
format_doc: Final = ...
_table_mod = ...
_table_file_open_policy_is_strict = ...
def to_hdf(path_or_buf: FilePath | HDFStore, key: str, value: DataFrame | Series, mode: str = ..., complevel: int | None = ..., complib: str | None = ..., append: bool = ..., format: str | None = ..., index: bool = ..., min_itemsize: int | dict[str, int] | None = ..., nan_rep=..., dropna: bool | None = ..., data_columns: Literal[True] | list[str] | None = ..., errors: str = ..., encoding: str = ...) -> None:
    """store this object, close it if we opened it"""
    ...

def read_hdf(path_or_buf: FilePath | HDFStore, key=..., mode: str = ..., errors: str = ..., where: str | list | None = ..., start: int | None = ..., stop: int | None = ..., columns: list[str] | None = ..., iterator: bool = ..., chunksize: int | None = ..., **kwargs): # -> TableIterator:
    """
    Read from the store, close it if we opened it.

    Retrieve pandas object stored in file, optionally based on where
    criteria.

    .. warning::

       Pandas uses PyTables for reading and writing HDF5 files, which allows
       serializing object-dtype data with pickle when using the "fixed" format.
       Loading pickled data received from untrusted sources can be unsafe.

       See: https://docs.python.org/3/library/pickle.html for more.

    Parameters
    ----------
    path_or_buf : str, path object, pandas.HDFStore
        Any valid string path is acceptable. Only supports the local file system,
        remote URLs and file-like objects are not supported.

        If you want to pass in a path object, pandas accepts any
        ``os.PathLike``.

        Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.

    key : object, optional
        The group identifier in the store. Can be omitted if the HDF file
        contains a single pandas object.
    mode : {'r', 'r+', 'a'}, default 'r'
        Mode to use when opening the file. Ignored if path_or_buf is a
        :class:`pandas.HDFStore`. Default is 'r'.
    errors : str, default 'strict'
        Specifies how encoding and decoding errors are to be handled.
        See the errors argument for :func:`open` for a full list
        of options.
    where : list, optional
        A list of Term (or convertible) objects.
    start : int, optional
        Row number to start selection.
    stop  : int, optional
        Row number to stop selection.
    columns : list, optional
        A list of columns names to return.
    iterator : bool, optional
        Return an iterator object.
    chunksize : int, optional
        Number of rows to include in an iteration when using an iterator.
    **kwargs
        Additional keyword arguments passed to HDFStore.

    Returns
    -------
    object
        The selected object. Return type depends on the object stored.

    See Also
    --------
    DataFrame.to_hdf : Write a HDF file from a DataFrame.
    HDFStore : Low-level access to HDF files.

    Notes
    -----
    When ``errors="surrogatepass"``, ``pd.options.future.infer_string`` is true,
    and PyArrow is installed, if a UTF-16 surrogate is encountered when decoding
    to UTF-8, the resulting dtype will be
    ``pd.StringDtype(storage="python", na_value=np.nan)``.

    Examples
    --------
    >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP
    >>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP
    >>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP
    """
    ...

class HDFStore:
    """
    Dict-like IO interface for storing pandas objects in PyTables.

    Either Fixed or Table format.

    .. warning::

       Pandas uses PyTables for reading and writing HDF5 files, which allows
       serializing object-dtype data with pickle when using the "fixed" format.
       Loading pickled data received from untrusted sources can be unsafe.

       See: https://docs.python.org/3/library/pickle.html for more.

    Parameters
    ----------
    path : str
        File path to HDF5 file.
    mode : {'a', 'w', 'r', 'r+'}, default 'a'

        ``'r'``
            Read-only; no data can be modified.
        ``'w'``
            Write; a new file is created (an existing file with the same
            name would be deleted).
        ``'a'``
            Append; an existing file is opened for reading and writing,
            and if the file does not exist it is created.
        ``'r+'``
            It is similar to ``'a'``, but the file must already exist.
    complevel : int, 0-9, default None
        Specifies a compression level for data.
        A value of 0 or None disables compression.
    complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'
        Specifies the compression library to be used.
        These additional compressors for Blosc are supported
        (default if no compressor specified: 'blosc:blosclz'):
        {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',
         'blosc:zlib', 'blosc:zstd'}.
        Specifying a compression library which is not available issues
        a ValueError.
    fletcher32 : bool, default False
        If applying compression use the fletcher32 checksum.
    **kwargs
        These parameters will be passed to the PyTables open_file method.

    Examples
    --------
    >>> bar = pd.DataFrame(np.random.randn(10, 4))
    >>> store = pd.HDFStore('test.h5')
    >>> store['foo'] = bar   # write to HDF5
    >>> bar = store['foo']   # retrieve
    >>> store.close()

    **Create or load HDF5 file in-memory**

    When passing the `driver` option to the PyTables open_file method through
    **kwargs, the HDF5 file is loaded or created in-memory and will only be
    written when closed:

    >>> bar = pd.DataFrame(np.random.randn(10, 4))
    >>> store = pd.HDFStore('test.h5', driver='H5FD_CORE')
    >>> store['foo'] = bar
    >>> store.close()   # only now, data is written to disk
    """
    _handle: File | None
    _mode: str
    def __init__(self, path, mode: str = ..., complevel: int | None = ..., complib=..., fletcher32: bool = ..., **kwargs) -> None:
        ...
    
    def __fspath__(self) -> str:
        ...
    
    @property
    def root(self):
        """return the root node"""
        ...
    
    @property
    def filename(self) -> str:
        ...
    
    def __getitem__(self, key: str):
        ...
    
    def __setitem__(self, key: str, value) -> None:
        ...
    
    def __delitem__(self, key: str) -> None:
        ...
    
    def __getattr__(self, name: str):
        """allow attribute access to get stores"""
        ...
    
    def __contains__(self, key: str) -> bool:
        """
        check for existence of this key
        can match the exact pathname or the pathnm w/o the leading '/'
        """
        ...
    
    def __len__(self) -> int:
        ...
    
    def __repr__(self) -> str:
        ...
    
    def __enter__(self) -> Self:
        ...
    
    def __exit__(self, exc_type: type[BaseException] | None, exc_value: BaseException | None, traceback: TracebackType | None) -> None:
        ...
    
    def keys(self, include: str = ...) -> list[str]:
        """
        Return a list of keys corresponding to objects stored in HDFStore.

        Parameters
        ----------

        include : str, default 'pandas'
                When kind equals 'pandas' return pandas objects.
                When kind equals 'native' return native HDF5 Table objects.

        Returns
        -------
        list
            List of ABSOLUTE path-names (e.g. have the leading '/').

        Raises
        ------
        raises ValueError if kind has an illegal value

        Examples
        --------
        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])
        >>> store = pd.HDFStore("store.h5", 'w')  # doctest: +SKIP
        >>> store.put('data', df)  # doctest: +SKIP
        >>> store.get('data')  # doctest: +SKIP
        >>> print(store.keys())  # doctest: +SKIP
        ['/data1', '/data2']
        >>> store.close()  # doctest: +SKIP
        """
        ...
    
    def __iter__(self) -> Iterator[str]:
        ...
    
    def items(self) -> Iterator[tuple[str, list]]:
        """
        iterate on key->group
        """
        ...
    
    def open(self, mode: str = ..., **kwargs) -> None:
        """
        Open the file in the specified mode

        Parameters
        ----------
        mode : {'a', 'w', 'r', 'r+'}, default 'a'
            See HDFStore docstring or tables.open_file for info about modes
        **kwargs
            These parameters will be passed to the PyTables open_file method.
        """
        ...
    
    def close(self) -> None:
        """
        Close the PyTables file handle
        """
        ...
    
    @property
    def is_open(self) -> bool:
        """
        return a boolean indicating whether the file is open
        """
        ...
    
    def flush(self, fsync: bool = ...) -> None:
        """
        Force all buffered modifications to be written to disk.

        Parameters
        ----------
        fsync : bool (default False)
          call ``os.fsync()`` on the file handle to force writing to disk.

        Notes
        -----
        Without ``fsync=True``, flushing may not guarantee that the OS writes
        to disk. With fsync, the operation will block until the OS claims the
        file has been written; however, other caching layers may still
        interfere.
        """
        ...
    
    def get(self, key: str):
        """
        Retrieve pandas object stored in file.

        Parameters
        ----------
        key : str

        Returns
        -------
        object
            Same type as object stored in file.

        Examples
        --------
        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])
        >>> store = pd.HDFStore("store.h5", 'w')  # doctest: +SKIP
        >>> store.put('data', df)  # doctest: +SKIP
        >>> store.get('data')  # doctest: +SKIP
        >>> store.close()  # doctest: +SKIP
        """
        ...
    
    def select(self, key: str, where=..., start=..., stop=..., columns=..., iterator: bool = ..., chunksize: int | None = ..., auto_close: bool = ...): # -> TableIterator:
        """
        Retrieve pandas object stored in file, optionally based on where criteria.

        .. warning::

           Pandas uses PyTables for reading and writing HDF5 files, which allows
           serializing object-dtype data with pickle when using the "fixed" format.
           Loading pickled data received from untrusted sources can be unsafe.

           See: https://docs.python.org/3/library/pickle.html for more.

        Parameters
        ----------
        key : str
            Object being retrieved from file.
        where : list or None
            List of Term (or convertible) objects, optional.
        start : int or None
            Row number to start selection.
        stop : int, default None
            Row number to stop selection.
        columns : list or None
            A list of columns that if not None, will limit the return columns.
        iterator : bool or False
            Returns an iterator.
        chunksize : int or None
            Number or rows to include in iteration, return an iterator.
        auto_close : bool or False
            Should automatically close the store when finished.

        Returns
        -------
        object
            Retrieved object from file.

        Examples
        --------
        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])
        >>> store = pd.HDFStore("store.h5", 'w')  # doctest: +SKIP
        >>> store.put('data', df)  # doctest: +SKIP
        >>> store.get('data')  # doctest: +SKIP
        >>> print(store.keys())  # doctest: +SKIP
        ['/data1', '/data2']
        >>> store.select('/data1')  # doctest: +SKIP
           A  B
        0  1  2
        1  3  4
        >>> store.select('/data1', where='columns == A')  # doctest: +SKIP
           A
        0  1
        1  3
        >>> store.close()  # doctest: +SKIP
        """
        ...
    
    def select_as_coordinates(self, key: str, where=..., start: int | None = ..., stop: int | None = ...): # -> Index | Literal[False]:
        """
        return the selection as an Index

        .. warning::

           Pandas uses PyTables for reading and writing HDF5 files, which allows
           serializing object-dtype data with pickle when using the "fixed" format.
           Loading pickled data received from untrusted sources can be unsafe.

           See: https://docs.python.org/3/library/pickle.html for more.


        Parameters
        ----------
        key : str
        where : list of Term (or convertible) objects, optional
        start : integer (defaults to None), row number to start selection
        stop  : integer (defaults to None), row number to stop selection
        """
        ...
    
    def select_column(self, key: str, column: str, start: int | None = ..., stop: int | None = ...): # -> Series | Literal[False]:
        """
        return a single column from the table. This is generally only useful to
        select an indexable

        .. warning::

           Pandas uses PyTables for reading and writing HDF5 files, which allows
           serializing object-dtype data with pickle when using the "fixed" format.
           Loading pickled data received from untrusted sources can be unsafe.

           See: https://docs.python.org/3/library/pickle.html for more.

        Parameters
        ----------
        key : str
        column : str
            The column of interest.
        start : int or None, default None
        stop : int or None, default None

        Raises
        ------
        raises KeyError if the column is not found (or key is not a valid
            store)
        raises ValueError if the column can not be extracted individually (it
            is part of a data block)

        """
        ...
    
    def select_as_multiple(self, keys, where=..., selector=..., columns=..., start=..., stop=..., iterator: bool = ..., chunksize: int | None = ..., auto_close: bool = ...): # -> TableIterator:
        """
        Retrieve pandas objects from multiple tables.

        .. warning::

           Pandas uses PyTables for reading and writing HDF5 files, which allows
           serializing object-dtype data with pickle when using the "fixed" format.
           Loading pickled data received from untrusted sources can be unsafe.

           See: https://docs.python.org/3/library/pickle.html for more.

        Parameters
        ----------
        keys : a list of the tables
        selector : the table to apply the where criteria (defaults to keys[0]
            if not supplied)
        columns : the columns I want back
        start : integer (defaults to None), row number to start selection
        stop  : integer (defaults to None), row number to stop selection
        iterator : bool, return an iterator, default False
        chunksize : nrows to include in iteration, return an iterator
        auto_close : bool, default False
            Should automatically close the store when finished.

        Raises
        ------
        raises KeyError if keys or selector is not found or keys is empty
        raises TypeError if keys is not a list or tuple
        raises ValueError if the tables are not ALL THE SAME DIMENSIONS
        """
        ...
    
    def put(self, key: str, value: DataFrame | Series, format=..., index: bool = ..., append: bool = ..., complib=..., complevel: int | None = ..., min_itemsize: int | dict[str, int] | None = ..., nan_rep=..., data_columns: Literal[True] | list[str] | None = ..., encoding=..., errors: str = ..., track_times: bool = ..., dropna: bool = ...) -> None:
        """
        Store object in HDFStore.

        Parameters
        ----------
        key : str
        value : {Series, DataFrame}
        format : 'fixed(f)|table(t)', default is 'fixed'
            Format to use when storing object in HDFStore. Value can be one of:

            ``'fixed'``
                Fixed format.  Fast writing/reading. Not-appendable, nor searchable.
            ``'table'``
                Table format.  Write as a PyTables Table structure which may perform
                worse but allow more flexible operations like searching / selecting
                subsets of the data.
        index : bool, default True
            Write DataFrame index as a column.
        append : bool, default False
            This will force Table format, append the input data to the existing.
        data_columns : list of columns or True, default None
            List of columns to create as data columns, or True to use all columns.
            See `here
            <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#query-via-data-columns>`__.
        encoding : str, default None
            Provide an encoding for strings.
        track_times : bool, default True
            Parameter is propagated to 'create_table' method of 'PyTables'.
            If set to False it enables to have the same h5 files (same hashes)
            independent on creation time.
        dropna : bool, default False, optional
            Remove missing values.

        Examples
        --------
        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])
        >>> store = pd.HDFStore("store.h5", 'w')  # doctest: +SKIP
        >>> store.put('data', df)  # doctest: +SKIP
        """
        ...
    
    def remove(self, key: str, where=..., start=..., stop=...) -> None:
        """
        Remove pandas object partially by specifying the where condition

        Parameters
        ----------
        key : str
            Node to remove or delete rows from
        where : list of Term (or convertible) objects, optional
        start : integer (defaults to None), row number to start selection
        stop  : integer (defaults to None), row number to stop selection

        Returns
        -------
        number of rows removed (or None if not a Table)

        Raises
        ------
        raises KeyError if key is not a valid store

        """
        ...
    
    def append(self, key: str, value: DataFrame | Series, format=..., axes=..., index: bool | list[str] = ..., append: bool = ..., complib=..., complevel: int | None = ..., columns=..., min_itemsize: int | dict[str, int] | None = ..., nan_rep=..., chunksize: int | None = ..., expectedrows=..., dropna: bool | None = ..., data_columns: Literal[True] | list[str] | None = ..., encoding=..., errors: str = ...) -> None:
        """
        Append to Table in file.

        Node must already exist and be Table format.

        Parameters
        ----------
        key : str
        value : {Series, DataFrame}
        format : 'table' is the default
            Format to use when storing object in HDFStore.  Value can be one of:

            ``'table'``
                Table format. Write as a PyTables Table structure which may perform
                worse but allow more flexible operations like searching / selecting
                subsets of the data.
        index : bool, default True
            Write DataFrame index as a column.
        append       : bool, default True
            Append the input data to the existing.
        data_columns : list of columns, or True, default None
            List of columns to create as indexed data columns for on-disk
            queries, or True to use all columns. By default only the axes
            of the object are indexed. See `here
            <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#query-via-data-columns>`__.
        min_itemsize : dict of columns that specify minimum str sizes
        nan_rep      : str to use as str nan representation
        chunksize    : size to chunk the writing
        expectedrows : expected TOTAL row size of this table
        encoding     : default None, provide an encoding for str
        dropna : bool, default False, optional
            Do not write an ALL nan row to the store settable
            by the option 'io.hdf.dropna_table'.

        Notes
        -----
        Does *not* check if data being appended overlaps with existing
        data in the table, so be careful

        Examples
        --------
        >>> df1 = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])
        >>> store = pd.HDFStore("store.h5", 'w')  # doctest: +SKIP
        >>> store.put('data', df1, format='table')  # doctest: +SKIP
        >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=['A', 'B'])
        >>> store.append('data', df2)  # doctest: +SKIP
        >>> store.close()  # doctest: +SKIP
           A  B
        0  1  2
        1  3  4
        0  5  6
        1  7  8
        """
        ...
    
    def append_to_multiple(self, d: dict, value, selector, data_columns=..., axes=..., dropna: bool = ..., **kwargs) -> None:
        """
        Append to multiple tables

        Parameters
        ----------
        d : a dict of table_name to table_columns, None is acceptable as the
            values of one node (this will get all the remaining columns)
        value : a pandas object
        selector : a string that designates the indexable table; all of its
            columns will be designed as data_columns, unless data_columns is
            passed, in which case these are used
        data_columns : list of columns to create as data columns, or True to
            use all columns
        dropna : if evaluates to True, drop rows from all tables if any single
                 row in each table has all NaN. Default False.

        Notes
        -----
        axes parameter is currently not accepted

        """
        ...
    
    def create_table_index(self, key: str, columns=..., optlevel: int | None = ..., kind: str | None = ...) -> None:
        """
        Create a pytables index on the table.

        Parameters
        ----------
        key : str
        columns : None, bool, or listlike[str]
            Indicate which columns to create an index on.

            * False : Do not create any indexes.
            * True : Create indexes on all columns.
            * None : Create indexes on all columns.
            * listlike : Create indexes on the given columns.

        optlevel : int or None, default None
            Optimization level, if None, pytables defaults to 6.
        kind : str or None, default None
            Kind of index, if None, pytables defaults to "medium".

        Raises
        ------
        TypeError: raises if the node is not a table
        """
        ...
    
    def groups(self) -> list:
        """
        Return a list of all the top-level nodes.

        Each node returned is not a pandas storage object.

        Returns
        -------
        list
            List of objects.

        Examples
        --------
        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])
        >>> store = pd.HDFStore("store.h5", 'w')  # doctest: +SKIP
        >>> store.put('data', df)  # doctest: +SKIP
        >>> print(store.groups())  # doctest: +SKIP
        >>> store.close()  # doctest: +SKIP
        [/data (Group) ''
          children := ['axis0' (Array), 'axis1' (Array), 'block0_values' (Array),
          'block0_items' (Array)]]
        """
        ...
    
    def walk(self, where: str = ...) -> Iterator[tuple[str, list[str], list[str]]]:
        """
        Walk the pytables group hierarchy for pandas objects.

        This generator will yield the group path, subgroups and pandas object
        names for each group.

        Any non-pandas PyTables objects that are not a group will be ignored.

        The `where` group itself is listed first (preorder), then each of its
        child groups (following an alphanumerical order) is also traversed,
        following the same procedure.

        Parameters
        ----------
        where : str, default "/"
            Group where to start walking.

        Yields
        ------
        path : str
            Full path to a group (without trailing '/').
        groups : list
            Names (strings) of the groups contained in `path`.
        leaves : list
            Names (strings) of the pandas objects contained in `path`.

        Examples
        --------
        >>> df1 = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])
        >>> store = pd.HDFStore("store.h5", 'w')  # doctest: +SKIP
        >>> store.put('data', df1, format='table')  # doctest: +SKIP
        >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=['A', 'B'])
        >>> store.append('data', df2)  # doctest: +SKIP
        >>> store.close()  # doctest: +SKIP
        >>> for group in store.walk():  # doctest: +SKIP
        ...     print(group)  # doctest: +SKIP
        >>> store.close()  # doctest: +SKIP
        """
        ...
    
    def get_node(self, key: str) -> Node | None:
        """return the node with the key or None if it does not exist"""
        ...
    
    def get_storer(self, key: str) -> GenericFixed | Table:
        """return the storer object for a key, raise if not in the file"""
        ...
    
    def copy(self, file, mode: str = ..., propindexes: bool = ..., keys=..., complib=..., complevel: int | None = ..., fletcher32: bool = ..., overwrite: bool = ...) -> HDFStore:
        """
        Copy the existing store to a new file, updating in place.

        Parameters
        ----------
        propindexes : bool, default True
            Restore indexes in copied file.
        keys : list, optional
            List of keys to include in the copy (defaults to all).
        overwrite : bool, default True
            Whether to overwrite (remove and replace) existing nodes in the new store.
        mode, complib, complevel, fletcher32 same as in HDFStore.__init__

        Returns
        -------
        open file handle of the new store
        """
        ...
    
    def info(self) -> str:
        """
        Print detailed information on the store.

        Returns
        -------
        str

        Examples
        --------
        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])
        >>> store = pd.HDFStore("store.h5", 'w')  # doctest: +SKIP
        >>> store.put('data', df)  # doctest: +SKIP
        >>> print(store.info())  # doctest: +SKIP
        >>> store.close()  # doctest: +SKIP
        <class 'pandas.io.pytables.HDFStore'>
        File path: store.h5
        /data    frame    (shape->[2,2])
        """
        ...
    


class TableIterator:
    """
    Define the iteration interface on a table

    Parameters
    ----------
    store : HDFStore
    s     : the referred storer
    func  : the function to execute the query
    where : the where of the query
    nrows : the rows to iterate on
    start : the passed start value (default is None)
    stop  : the passed stop value (default is None)
    iterator : bool, default False
        Whether to use the default iterator.
    chunksize : the passed chunking value (default is 100000)
    auto_close : bool, default False
        Whether to automatically close the store at the end of iteration.
    """
    chunksize: int | None
    store: HDFStore
    s: GenericFixed | Table
    def __init__(self, store: HDFStore, s: GenericFixed | Table, func, where, nrows, start=..., stop=..., iterator: bool = ..., chunksize: int | None = ..., auto_close: bool = ...) -> None:
        ...
    
    def __iter__(self) -> Iterator:
        ...
    
    def close(self) -> None:
        ...
    
    def get_result(self, coordinates: bool = ...): # -> Self:
        ...
    


class IndexCol:
    """
    an index column description class

    Parameters
    ----------
    axis   : axis which I reference
    values : the ndarray like converted values
    kind   : a string description of this type
    typ    : the pytables type
    pos    : the position in the pytables

    """
    is_an_indexable: bool = ...
    is_data_indexable: bool = ...
    _info_fields = ...
    def __init__(self, name: str, values=..., kind=..., typ=..., cname: str | None = ..., axis=..., pos=..., freq=..., tz=..., index_name=..., ordered=..., table=..., meta=..., metadata=...) -> None:
        ...
    
    @property
    def itemsize(self) -> int:
        ...
    
    @property
    def kind_attr(self) -> str:
        ...
    
    def set_pos(self, pos: int) -> None:
        """set the position of this column in the Table"""
        ...
    
    def __repr__(self) -> str:
        ...
    
    def __eq__(self, other: object) -> bool:
        """compare 2 col items"""
        ...
    
    def __ne__(self, other) -> bool:
        ...
    
    @property
    def is_indexed(self) -> bool:
        """return whether I am an indexed column"""
        ...
    
    def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str) -> tuple[np.ndarray, np.ndarray] | tuple[Index, Index]:
        """
        Convert the data from this selection to the appropriate pandas type.
        """
        ...
    
    def take_data(self): # -> None:
        """return the values"""
        ...
    
    @property
    def attrs(self): # -> Any:
        ...
    
    @property
    def description(self): # -> Any:
        ...
    
    @property
    def col(self): # -> Any | None:
        """return my current col description"""
        ...
    
    @property
    def cvalues(self): # -> None:
        """return my cython values"""
        ...
    
    def __iter__(self) -> Iterator:
        ...
    
    def maybe_set_size(self, min_itemsize=...) -> None:
        """
        maybe set a string col itemsize:
            min_itemsize can be an integer or a dict with this columns name
            with an integer size
        """
        ...
    
    def validate_names(self) -> None:
        ...
    
    def validate_and_set(self, handler: AppendableTable, append: bool) -> None:
        ...
    
    def validate_col(self, itemsize=...): # -> Any | None:
        """validate this column: return the compared against itemsize"""
        ...
    
    def validate_attr(self, append: bool) -> None:
        ...
    
    def update_info(self, info) -> None:
        """
        set/update the info for this indexable with the key/value
        if there is a conflict raise/warn as needed
        """
        ...
    
    def set_info(self, info) -> None:
        """set my state from the passed info"""
        ...
    
    def set_attr(self) -> None:
        """set the kind for this column"""
        ...
    
    def validate_metadata(self, handler: AppendableTable) -> None:
        """validate that kind=category does not change the categories"""
        ...
    
    def write_metadata(self, handler: AppendableTable) -> None:
        """set the meta data"""
        ...
    


class GenericIndexCol(IndexCol):
    """an index which is not represented in the data of the table"""
    @property
    def is_indexed(self) -> bool:
        ...
    
    def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str) -> tuple[Index, Index]:
        """
        Convert the data from this selection to the appropriate pandas type.

        Parameters
        ----------
        values : np.ndarray
        nan_rep : str
        encoding : str
        errors : str
        """
        ...
    
    def set_attr(self) -> None:
        ...
    


class DataCol(IndexCol):
    """
    a data holding column, by definition this is not indexable

    Parameters
    ----------
    data   : the actual data
    cname  : the column name in the table to hold the data (typically
                values)
    meta   : a string description of the metadata
    metadata : the actual metadata
    """
    is_an_indexable = ...
    is_data_indexable = ...
    _info_fields = ...
    def __init__(self, name: str, values=..., kind=..., typ=..., cname: str | None = ..., pos=..., tz=..., ordered=..., table=..., meta=..., metadata=..., dtype: DtypeArg | None = ..., data=...) -> None:
        ...
    
    @property
    def dtype_attr(self) -> str:
        ...
    
    @property
    def meta_attr(self) -> str:
        ...
    
    def __repr__(self) -> str:
        ...
    
    def __eq__(self, other: object) -> bool:
        """compare 2 col items"""
        ...
    
    def set_data(self, data: ArrayLike) -> None:
        ...
    
    def take_data(self): # -> ndarray[_AnyShape, dtype[Any]] | None:
        """return the data"""
        ...
    
    @classmethod
    def get_atom_string(cls, shape, itemsize):
        ...
    
    @classmethod
    def get_atom_coltype(cls, kind: str) -> type[Col]:
        """return the PyTables column class for this column"""
        ...
    
    @classmethod
    def get_atom_data(cls, shape, kind: str) -> Col:
        ...
    
    @classmethod
    def get_atom_datetime64(cls, shape):
        ...
    
    @classmethod
    def get_atom_timedelta64(cls, shape):
        ...
    
    @property
    def shape(self): # -> Any | None:
        ...
    
    @property
    def cvalues(self): # -> ndarray[_AnyShape, dtype[Any]] | None:
        """return my cython values"""
        ...
    
    def validate_attr(self, append) -> None:
        """validate that we have the same order as the existing & same dtype"""
        ...
    
    def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str): # -> tuple[Any | None, ndarray[_AnyShape, dtype[Any]] | Categorical | Any]:
        """
        Convert the data from this selection to the appropriate pandas type.

        Parameters
        ----------
        values : np.ndarray
        nan_rep :
        encoding : str
        errors : str

        Returns
        -------
        index : listlike to become an Index
        data : ndarraylike to become a column
        """
        ...
    
    def set_attr(self) -> None:
        """set the data for this column"""
        ...
    


class DataIndexableCol(DataCol):
    """represent a data column that can be indexed"""
    is_data_indexable = ...
    def validate_names(self) -> None:
        ...
    
    @classmethod
    def get_atom_string(cls, shape, itemsize):
        ...
    
    @classmethod
    def get_atom_data(cls, shape, kind: str) -> Col:
        ...
    
    @classmethod
    def get_atom_datetime64(cls, shape):
        ...
    
    @classmethod
    def get_atom_timedelta64(cls, shape):
        ...
    


class GenericDataIndexableCol(DataIndexableCol):
    """represent a generic pytables data column"""
    ...


class Fixed:
    """
    represent an object in my store
    facilitate read/write of various types of objects
    this is an abstract base class

    Parameters
    ----------
    parent : HDFStore
    group : Node
        The group node where the table resides.
    """
    pandas_kind: str
    format_type: str = ...
    obj_type: type[DataFrame | Series]
    ndim: int
    parent: HDFStore
    is_table: bool = ...
    def __init__(self, parent: HDFStore, group: Node, encoding: str | None = ..., errors: str = ...) -> None:
        ...
    
    @property
    def is_old_version(self) -> bool:
        ...
    
    @property
    def version(self) -> tuple[int, int, int]:
        """compute and set our version"""
        ...
    
    @property
    def pandas_type(self): # -> str | Any | None:
        ...
    
    def __repr__(self) -> str:
        """return a pretty representation of myself"""
        ...
    
    def set_object_info(self) -> None:
        """set my pandas type & version"""
        ...
    
    def copy(self) -> Fixed:
        ...
    
    @property
    def shape(self): # -> Any | None:
        ...
    
    @property
    def pathname(self):
        ...
    
    @property
    def attrs(self):
        ...
    
    def set_attrs(self) -> None:
        """set our object attributes"""
        ...
    
    def get_attrs(self) -> None:
        """get our object attributes"""
        ...
    
    @property
    def storable(self):
        """return my storable"""
        ...
    
    @property
    def is_exists(self) -> bool:
        ...
    
    @property
    def nrows(self): # -> Any | None:
        ...
    
    def validate(self, other) -> Literal[True] | None:
        """validate against an existing storable"""
        ...
    
    def validate_version(self, where=...) -> None:
        """are we trying to operate on an old version?"""
        ...
    
    def infer_axes(self) -> bool:
        """
        infer the axes of my storer
        return a boolean indicating if we have a valid storer or not
        """
        ...
    
    def read(self, where=..., columns=..., start: int | None = ..., stop: int | None = ...):
        ...
    
    def write(self, obj, **kwargs) -> None:
        ...
    
    def delete(self, where=..., start: int | None = ..., stop: int | None = ...) -> None:
        """
        support fully deleting the node in its entirety (only) - where
        specification must be None
        """
        ...
    


class GenericFixed(Fixed):
    """a generified fixed version"""
    _index_type_map = ...
    _reverse_index_map = ...
    attributes: list[str] = ...
    def validate_read(self, columns, where) -> None:
        """
        raise if any keywords are passed which are not-None
        """
        ...
    
    @property
    def is_exists(self) -> bool:
        ...
    
    def set_attrs(self) -> None:
        """set our object attributes"""
        ...
    
    def get_attrs(self) -> None:
        """retrieve our attributes"""
        ...
    
    def write(self, obj, **kwargs) -> None:
        ...
    
    def read_array(self, key: str, start: int | None = ..., stop: int | None = ...): # -> ExtensionArray | Any | ndarray[_AnyShape, dtype[Any]] | _Array1D[Any]:
        """read an array for the specified node (off of group"""
        ...
    
    def read_index(self, key: str, start: int | None = ..., stop: int | None = ...) -> Index:
        ...
    
    def write_index(self, key: str, index: Index) -> None:
        ...
    
    def write_multi_index(self, key: str, index: MultiIndex) -> None:
        ...
    
    def read_multi_index(self, key: str, start: int | None = ..., stop: int | None = ...) -> MultiIndex:
        ...
    
    def read_index_node(self, node: Node, start: int | None = ..., stop: int | None = ...) -> Index:
        ...
    
    def write_array_empty(self, key: str, value: ArrayLike) -> None:
        """write a 0-len array"""
        ...
    
    def write_array(self, key: str, obj: AnyArrayLike, items: Index | None = ...) -> None:
        ...
    


class SeriesFixed(GenericFixed):
    pandas_kind = ...
    attributes = ...
    name: Hashable
    @property
    def shape(self): # -> tuple[int] | None:
        ...
    
    def read(self, where=..., columns=..., start: int | None = ..., stop: int | None = ...) -> Series:
        ...
    
    def write(self, obj, **kwargs) -> None:
        ...
    


class BlockManagerFixed(GenericFixed):
    attributes = ...
    nblocks: int
    @property
    def shape(self) -> Shape | None:
        ...
    
    def read(self, where=..., columns=..., start: int | None = ..., stop: int | None = ...) -> DataFrame:
        ...
    
    def write(self, obj, **kwargs) -> None:
        ...
    


class FrameFixed(BlockManagerFixed):
    pandas_kind = ...
    obj_type = ...


class Table(Fixed):
    """
    represent a table:
        facilitate read/write of various types of tables

    Attrs in Table Node
    -------------------
    These are attributes that are store in the main table node, they are
    necessary to recreate these tables when read back in.

    index_axes    : a list of tuples of the (original indexing axis and
        index column)
    non_index_axes: a list of tuples of the (original index axis and
        columns on a non-indexing axis)
    values_axes   : a list of the columns which comprise the data of this
        table
    data_columns  : a list of the columns that we are allowing indexing
        (these become single columns in values_axes)
    nan_rep       : the string to use for nan representations for string
        objects
    levels        : the names of levels
    metadata      : the names of the metadata columns
    """
    pandas_kind = ...
    format_type: str = ...
    table_type: str
    levels: int | list[Hashable] = ...
    is_table = ...
    metadata: list
    def __init__(self, parent: HDFStore, group: Node, encoding: str | None = ..., errors: str = ..., index_axes: list[IndexCol] | None = ..., non_index_axes: list[tuple[AxisInt, Any]] | None = ..., values_axes: list[DataCol] | None = ..., data_columns: list | None = ..., info: dict | None = ..., nan_rep=...) -> None:
        ...
    
    @property
    def table_type_short(self) -> str:
        ...
    
    def __repr__(self) -> str:
        """return a pretty representation of myself"""
        ...
    
    def __getitem__(self, c: str): # -> IndexCol | None:
        """return the axis for c"""
        ...
    
    def validate(self, other) -> None:
        """validate against an existing table"""
        ...
    
    @property
    def is_multi_index(self) -> bool:
        """the levels attribute is 1 or a list in the case of a multi-index"""
        ...
    
    def validate_multiindex(self, obj: DataFrame | Series) -> tuple[DataFrame, list[Hashable]]:
        """
        validate that we can store the multi-index; reset and return the
        new object
        """
        ...
    
    @property
    def nrows_expected(self) -> int:
        """based on our axes, compute the expected nrows"""
        ...
    
    @property
    def is_exists(self) -> bool:
        """has this table been created"""
        ...
    
    @property
    def storable(self): # -> Any | None:
        ...
    
    @property
    def table(self): # -> Any | None:
        """return the table group (this is my storable)"""
        ...
    
    @property
    def dtype(self): # -> Any:
        ...
    
    @property
    def description(self): # -> Any:
        ...
    
    @property
    def axes(self) -> itertools.chain[IndexCol]:
        ...
    
    @property
    def ncols(self) -> int:
        """the number of total columns in the values axes"""
        ...
    
    @property
    def is_transposed(self) -> bool:
        ...
    
    @property
    def data_orientation(self) -> tuple[int, ...]:
        """return a tuple of my permutated axes, non_indexable at the front"""
        ...
    
    def queryables(self) -> dict[str, Any]:
        """return a dict of the kinds allowable columns for this object"""
        ...
    
    def index_cols(self): # -> list[tuple[Any | None, str | Any]]:
        """return a list of my index cols"""
        ...
    
    def values_cols(self) -> list[str]:
        """return a list of my values cols"""
        ...
    
    def write_metadata(self, key: str, values: np.ndarray) -> None:
        """
        Write out a metadata array to the key as a fixed-format Series.

        Parameters
        ----------
        key : str
        values : ndarray
        """
        ...
    
    def read_metadata(self, key: str): # -> TableIterator | None:
        """return the meta data array for this key"""
        ...
    
    def set_attrs(self) -> None:
        """set our table type & indexables"""
        ...
    
    def get_attrs(self) -> None:
        """retrieve our attributes"""
        ...
    
    def validate_version(self, where=...) -> None:
        """are we trying to operate on an old version?"""
        ...
    
    def validate_min_itemsize(self, min_itemsize) -> None:
        """
        validate the min_itemsize doesn't contain items that are not in the
        axes this needs data_columns to be defined
        """
        ...
    
    @cache_readonly
    def indexables(self): # -> list[Any]:
        """create/cache the indexables if they don't exist"""
        ...
    
    def create_index(self, columns=..., optlevel=..., kind: str | None = ...) -> None:
        """
        Create a pytables index on the specified columns.

        Parameters
        ----------
        columns : None, bool, or listlike[str]
            Indicate which columns to create an index on.

            * False : Do not create any indexes.
            * True : Create indexes on all columns.
            * None : Create indexes on all columns.
            * listlike : Create indexes on the given columns.

        optlevel : int or None, default None
            Optimization level, if None, pytables defaults to 6.
        kind : str or None, default None
            Kind of index, if None, pytables defaults to "medium".

        Raises
        ------
        TypeError if trying to create an index on a complex-type column.

        Notes
        -----
        Cannot index Time64Col or ComplexCol.
        Pytables must be >= 3.0.
        """
        ...
    
    @classmethod
    def get_object(cls, obj, transposed: bool):
        """return the data for this obj"""
        ...
    
    def validate_data_columns(self, data_columns, min_itemsize, non_index_axes): # -> list[Any]:
        """
        take the input data_columns and min_itemize and create a data
        columns spec
        """
        ...
    
    def process_axes(self, obj, selection: Selection, columns=...) -> DataFrame:
        """process axes filters"""
        ...
    
    def create_description(self, complib, complevel: int | None, fletcher32: bool, expectedrows: int | None) -> dict[str, Any]:
        """create the description of the table from the axes & values"""
        ...
    
    def read_coordinates(self, where=..., start: int | None = ..., stop: int | None = ...): # -> Index | Literal[False]:
        """
        select coordinates (row numbers) from a table; return the
        coordinates object
        """
        ...
    
    def read_column(self, column: str, where=..., start: int | None = ..., stop: int | None = ...): # -> Series | Literal[False]:
        """
        return a single column from the table, generally only indexables
        are interesting
        """
        ...
    


class WORMTable(Table):
    """
    a write-once read-many table: this format DOES NOT ALLOW appending to a
    table. writing is a one-time operation the data are stored in a format
    that allows for searching the data on disk
    """
    table_type = ...
    def read(self, where=..., columns=..., start: int | None = ..., stop: int | None = ...):
        """
        read the indices and the indexing array, calculate offset rows and return
        """
        ...
    
    def write(self, obj, **kwargs) -> None:
        """
        write in a format that we can search later on (but cannot append
        to): write out the indices and the values using _write_array
        (e.g. a CArray) create an indexing table so that we can search
        """
        ...
    


class AppendableTable(Table):
    """support the new appendable table formats"""
    table_type = ...
    def write(self, obj, axes=..., append: bool = ..., complib=..., complevel=..., fletcher32=..., min_itemsize=..., chunksize: int | None = ..., expectedrows=..., dropna: bool = ..., nan_rep=..., data_columns=..., track_times: bool = ...) -> None:
        ...
    
    def write_data(self, chunksize: int | None, dropna: bool = ...) -> None:
        """
        we form the data into a 2-d including indexes,values,mask write chunk-by-chunk
        """
        ...
    
    def write_data_chunk(self, rows: np.ndarray, indexes: list[np.ndarray], mask: npt.NDArray[np.bool_] | None, values: list[np.ndarray]) -> None:
        """
        Parameters
        ----------
        rows : an empty memory space where we are putting the chunk
        indexes : an array of the indexes
        mask : an array of the masks
        values : an array of the values
        """
        ...
    
    def delete(self, where=..., start: int | None = ..., stop: int | None = ...): # -> Any | int | None:
        ...
    


class AppendableFrameTable(AppendableTable):
    """support the new appendable table formats"""
    pandas_kind = ...
    table_type = ...
    ndim = ...
    obj_type: type[DataFrame | Series] = ...
    @property
    def is_transposed(self) -> bool:
        ...
    
    @classmethod
    def get_object(cls, obj, transposed: bool):
        """these are written transposed"""
        ...
    
    def read(self, where=..., columns=..., start: int | None = ..., stop: int | None = ...): # -> DataFrame | None:
        ...
    


class AppendableSeriesTable(AppendableFrameTable):
    """support the new appendable table formats"""
    pandas_kind = ...
    table_type = ...
    ndim = ...
    obj_type = ...
    @property
    def is_transposed(self) -> bool:
        ...
    
    @classmethod
    def get_object(cls, obj, transposed: bool):
        ...
    
    def write(self, obj, data_columns=..., **kwargs) -> None:
        """we are going to write this as a frame table"""
        ...
    
    def read(self, where=..., columns=..., start: int | None = ..., stop: int | None = ...) -> Series:
        ...
    


class AppendableMultiSeriesTable(AppendableSeriesTable):
    """support the new appendable table formats"""
    pandas_kind = ...
    table_type = ...
    def write(self, obj, **kwargs) -> None:
        """we are going to write this as a frame table"""
        ...
    


class GenericTable(AppendableFrameTable):
    """a table that read/writes the generic pytables table format"""
    pandas_kind = ...
    table_type = ...
    ndim = ...
    obj_type = ...
    levels: list[Hashable]
    @property
    def pandas_type(self) -> str:
        ...
    
    @property
    def storable(self): # -> Any:
        ...
    
    def get_attrs(self) -> None:
        """retrieve our attributes"""
        ...
    
    @cache_readonly
    def indexables(self): # -> list[GenericIndexCol | GenericDataIndexableCol]:
        """create the indexables from the table description"""
        ...
    
    def write(self, **kwargs) -> None:
        ...
    


class AppendableMultiFrameTable(AppendableFrameTable):
    """a frame with a multi-index"""
    table_type = ...
    obj_type = ...
    ndim = ...
    _re_levels = ...
    @property
    def table_type_short(self) -> str:
        ...
    
    def write(self, obj, data_columns=..., **kwargs) -> None:
        ...
    
    def read(self, where=..., columns=..., start: int | None = ..., stop: int | None = ...): # -> DataFrame:
        ...
    


class Selection:
    """
    Carries out a selection operation on a tables.Table object.

    Parameters
    ----------
    table : a Table object
    where : list of Terms (or convertible to)
    start, stop: indices to start and/or stop selection

    """
    def __init__(self, table: Table, where=..., start: int | None = ..., stop: int | None = ...) -> None:
        ...
    
    def generate(self, where): # -> PyTablesExpr | None:
        """where can be a : dict,list,tuple,string"""
        ...
    
    def select(self): # -> Any:
        """
        generate the selection
        """
        ...
    
    def select_coords(self): # -> Any | ndarray[_AnyShape, dtype[Any]] | NDArray[Any] | _Array1D[Any]:
        """
        generate the selection
        """
        ...
    


